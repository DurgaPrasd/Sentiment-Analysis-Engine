{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # provide sql-like data manipulation tools. very handy.\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np # high dimensional vector computing library.\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "import json\n",
    "from pprint import pprint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from numpy.random import RandomState\n",
    "prng = RandomState(1234567890)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\mites\\Documents\\Cognitive\\Midterm\\Rawdata\\ssix-project-semeval-2017-task-5-subtask-2-2fb645e839b7\\Headline_Trainingdata.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "pprint(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\mites\\Documents\\Cognitive\\Midterm\\Rawdata\\ssix-project-semeval-2017-task-5-subtask-2-2fb645e839b7\\Headlines_Testdata.json',encoding=\"utf8\") as data_files:    \n",
    "    datatest = json.load(data_files)\n",
    "\n",
    "pprint(datatest[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = [x[\"id\"] for x in data]\n",
    "company = [x[\"company\"] for x in data]\n",
    "title = [x[\"title\"] for x in data]\n",
    "score = [x[\"sentiment\"] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result= list(map(float, score))\n",
    "sentscr=[]\n",
    "for item in result:\n",
    "   \n",
    "    if item>=-1 and item<=-0.3:\n",
    "        sentscr.append(-1)\n",
    "    elif item>-0.3 and item<=0.3:\n",
    "        sentscr.append(0)\n",
    "    else: \n",
    "        sentscr.append(1)\n",
    "        \n",
    "sentscr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {'id':ids,\n",
    "     'company': company,\n",
    "     'sentimentscore': sentscr,\n",
    "    })\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(\n",
    "    {'title':title\n",
    "    })\n",
    "df1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = df1.title.astype(str).str.replace('\\[|\\]|\\'', '')\n",
    "a[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2=a.to_frame()\n",
    "df2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = df.join(df2)\n",
    "dataframe[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = [x[\"id\"] for x in datatest]\n",
    "company = [x[\"company\"] for x in datatest]\n",
    "title = [x[\"title\"] for x in datatest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dft = pd.DataFrame(\n",
    "    {'id':ids,\n",
    "     'company': company,\n",
    "    })\n",
    "dft[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dft1 = pd.DataFrame(\n",
    "    {'title':title\n",
    "    })\n",
    "dft1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = dft1.title.astype(str).str.replace('\\[|\\]|\\'', '')\n",
    "b[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dft2=b.to_frame()\n",
    "dft2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdataframe = dft.join(dft2)\n",
    "testdataframe[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdataframe = testdataframe[['id','company','title']]\n",
    "testdataframe[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = 'C:\\\\Users\\mites\\Documents\\Cognitive\\Midterm'\n",
    "GLOVE_DIR = BASE_DIR + '\\GoogleNews-vectors-negative300'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 3000\n",
    "MAX_NB_WORDS = 20000\n",
    "\n",
    "# Consider changing the 200 to 25\n",
    "EMBEDDING_DIM = 300\n",
    "GLOVE_FILE = 'C:\\\\Users\\mites\\Documents\\Cognitive\\Midterm\\GoogleNews-vectors-negative300\\GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {} # word -> coefs\n",
    "# We'll use the 100-dimensional version\n",
    "#f = open(os.path.join(GLOVE_DIR, GLOVE_FILE), encoding=\"utf8\")\n",
    "word2vec = KeyedVectors.load_word2vec_format(GLOVE_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Selecting the columns except the last one i.e sentiment score\n",
    "X = dataframe.iloc[:,3].values\n",
    "Xt = testdataframe.iloc[:,3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "data = pad_sequences(sequences, maxlen=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(Xt)\n",
    "sequences = tokenizer.texts_to_sequences(Xt)\n",
    "testdata = pad_sequences(sequences, maxlen=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = tokenizer.word_index\n",
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.array(dataframe.iloc[:,2].values)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data,Y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.seed(seed) # same shuffle for labels!\n",
    "np.random.shuffle(y_train)\n",
    "\n",
    "np.random.seed(seed * 2)\n",
    "np.random.shuffle(x_test)\n",
    "np.random.seed(seed * 2)\n",
    "np.random.shuffle(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Pad sequences')\n",
    "x_train = pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train[x_train >= MAX_NB_WORDS] = MAX_NB_WORDS-1\n",
    "x_test[x_test >= MAX_NB_WORDS] = MAX_NB_WORDS-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word2index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word2index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train a regular MLP\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Flatten()(embedded_sequences)\n",
    "x = Dense(25, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(25, activation='relu')(x)\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model_mlp = Model(sequence_input, preds)\n",
    "model_mlp.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training model.')\n",
    "history = model_mlp.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "          epochs=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model_mlp.predict(testdata, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abc = preds.astype(int).flatten().flatten()\n",
    "zxy = pd.DataFrame({'predictedvalues': abc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zxy[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
